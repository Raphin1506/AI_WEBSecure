#!/usr/bin/env python3
"""CyberSentinel¬†AI ‚Äì Streamlit dashboard (rev.‚ÄØ2025‚Äë07‚Äë30)

Principais mudan√ßas nesta revis√£o
--------------------------------
* **Thread resiliente**¬†‚Äì checa `is_alive()` e reinicia se necess√°rio.
* **Erros vis√≠veis**¬†‚Äì exce√ß√µes no worker v√£o para `st.session_state['capture_error']`.
* **Stop limpa tudo**¬†‚Äì encerra `LiveCapture.close()` e `join()` da thread.
* **Diagn√≥stico na UI**¬†‚Äì mostra status da thread, tamanho da fila e √∫ltima falha.
"""

from __future__ import annotations

import asyncio
import datetime as _dt
import queue
import threading
import sys
from collections import deque
from pathlib import Path
from pyshark.packet.packet import Packet
import pandas as pd
import streamlit as st
from sklearn.ensemble import IsolationForest
from io import StringIO
###########################################################################
# Compat / setup                                                          #
###########################################################################

if sys.platform.startswith("win"):
    try:
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    except AttributeError:
        pass

try:
    from streamlit_autorefresh import st_autorefresh  # type: ignore
except ModuleNotFoundError:  # graceful fallback
    def st_autorefresh(*_, **__):
        pass

# Monkey‚Äëpatch: make asyncio.all_tasks safe when no loop is running (Py3.11+)
_original_all_tasks = asyncio.all_tasks  # type: ignore

def _safe_all_tasks(loop=None):  # type: ignore
    try:
        return _original_all_tasks(loop)
    except RuntimeError:
        return set()

asyncio.all_tasks = _safe_all_tasks  # type: ignore

try:
    import pyshark  # needs Wireshark/tshark + Npcap
except ModuleNotFoundError:
    pyshark = None  # type: ignore

###########################################################################
# Live‚Äëcapture backend                                                    #
###########################################################################

PACKET_QUEUE: "queue.Queue[dict]" = queue.Queue(maxsize=5_000)


def _packet_to_row(pkt: Packet) -> dict:
    """Extract minimal fields needed for anomaly detection."""
    try:
        proto = pkt.transport_layer or "N/A"
    except AttributeError:
        proto = "N/A"
    try:
        length = int(getattr(pkt, "length", 0))
    except Exception:
        length = 0
    row = {
        "timestamp": _dt.datetime.now(),
        "ip_src": getattr(pkt, "ip", {}).src if hasattr(pkt, "ip") else "?",
        "ip_dst": getattr(pkt, "ip", {}).dst if hasattr(pkt, "ip") else "?",
        "proto": proto,
        "src_port": getattr(pkt[proto], "srcport", None) if proto != "N/A" else None,
        "dst_port": getattr(pkt[proto], "dstport", None) if proto != "N/A" else None,
        "size": length,
    }
    return row


def start_live_capture(interface: str = "1", bpf: str = "ip or ip6") -> None:
    """Spin up / restart a background PyShark capture thread."""
    if pyshark is None:
        st.error("PyShark/Wireshark n√£o instalado ‚Äì n√£o √© poss√≠vel capturar pacotes.")
        return

    # If an old thread exists but is dead ‚Üí clean reference so we can restart
    t = st.session_state.get("capture_thread")
    if t is not None and t.is_alive():
        return  # already running
    st.session_state["capture_thread"] = None  # drop stale
    st.session_state["capture_error"] = ""

    def _worker() -> None:
        asyncio.set_event_loop(asyncio.new_event_loop())
        try:
            cap = pyshark.LiveCapture(interface=interface, bpf_filter=bpf)
            st.session_state["live_cap"] = cap  # store to allow close()
            for pkt in cap.sniff_continuously():
                try:
                    PACKET_QUEUE.put_nowait(_packet_to_row(pkt))
                except queue.Full:
                    try:
                        PACKET_QUEUE.get_nowait()  # drop oldest
                    except queue.Empty:
                        pass
                    PACKET_QUEUE.put_nowait(_packet_to_row(pkt))
        except Exception as e:  # noqa: BLE001 ‚Äì surface any error
            st.session_state["capture_error"] = f"{type(e).__name__}: {e}"
        finally:
            st.session_state.pop("live_cap", None)

    t = threading.Thread(target=_worker, daemon=True, name="LiveCaptureThread")
    t.start()
    st.session_state["capture_thread"] = t


def stop_live_capture():
    """Terminate capture: close tshark and join thread."""
    # Close LiveCapture (kills tshark subprocess)
    cap = st.session_state.pop("live_cap", None)
    if cap is not None:
        try:
            cap.close()
        except Exception:
            pass

    # Join worker thread
    t = st.session_state.pop("capture_thread", None)
    if t is not None and t.is_alive():
        t.join(timeout=1.0)

###########################################################################
# Anomaly detection                                                       #
###########################################################################

def detectar_anomalias(df, colunas):
    modelo = IsolationForest(contamination=0.05, random_state=42)
    df['score'] = modelo.fit_predict(df[colunas])
    df['anomalia'] = modelo.decision_function(df[colunas])
    df['risco'] = df['anomalia'].apply(classificar_risco)
    df['acao_recomendada'] = df['acao'].apply(sugerir_acao)
    return df[df['score'] == -1]  # Somente anomalias

# Dicion√°rio de recomenda√ß√µes
RECOMENDACOES = {
    "SSH": "Verificar tentativas de brute force e bloquear IP se necess√°rio.",
    "RDP": "Avaliar necessidade de acesso remoto e aplicar autentica√ß√£o forte.",
    "FTP": "Desabilitar FTP se n√£o for necess√°rio; usar SFTP como alternativa segura.",
    "SMB": "Verificar exposi√ß√£o da porta 445 e aplicar patches de seguran√ßa.",
    "POST": "Monitorar payloads recebidos ‚Äî pode indicar tentativa de inje√ß√£o ou exfiltra√ß√£o.",
    "MySQL": "Verificar acessos n√£o autorizados e revisar credenciais do banco de dados.",
    "GET": "Monitorar volume de requisi√ß√µes para evitar poss√≠veis ataques de scraping ou DDoS."
}

def sugerir_acao(acao):
    return RECOMENDACOES.get(acao, "Investigar atividade incomum e revisar pol√≠ticas de acesso.")

def classificar_risco(score):
    if score < -0.25:
        return "ALTO"
    elif score < -0.10:
        return "M√âDIO"
    else:
        return "BAIXO"

def gerar_relatorio_texto(df) -> str:
                        buf = StringIO()
                        data = _dt.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                        buf.write(f"üõ°Ô∏è Relat√≥rio de Seguran√ßa - {data}\n\n")
                        for _, row in df.iterrows():
                            buf.write(f"üìÖ Data/Hora: {row.get('timestamp', '-')}\n")
                            buf.write(f"üåê IP: {row.get('ip', row.get('ip_src', '-'))}\n")
                            buf.write(f"üîê A√ß√£o: {row.get('acao', row.get('proto', '-'))}\n")
                            buf.write(f"‚ö†Ô∏è Score de Anomalia: {row['anomalia']:.4f}\n")
                            buf.write(f"üö® Risco: {row['risco']}\n")
                            buf.write(f"üí° A√ß√£o Recomendada: {row['acao_recomendada']}\n")
                            buf.write("-" * 40 + "\n")
                        return buf.getvalue()

###########################################################################
# Streamlit UI                                                            #
###########################################################################

st.set_page_config(page_title="CyberSentinel¬†AI", layout="wide")
st.title("üõ°Ô∏è CyberSentinel¬†AI ‚Äî Network Threat Monitor")

st.sidebar.header("üéõÔ∏è Controles")

interface_input = st.sidebar.text_input(
    "Interface",
    value="1",
    help="Use o N√öMERO mostrado por 'tshark -D' ou o GUID completo.",
)
bpf_filter = st.sidebar.text_input("BPF filter", value="ip or ip6")

t = st.session_state.get("capture_thread")
is_running = t is not None and t.is_alive()

col_start, col_stop = st.sidebar.columns(2)
if col_start.button("‚ñ∂Ô∏è Start", disabled=is_running):
    start_live_capture(interface_input, bpf_filter)

if col_stop.button("‚èπÔ∏è Stop", disabled=not is_running):
    stop_live_capture()

st.sidebar.markdown("---")
st.sidebar.info("Upload um CSV (exportado do Suricata, Zeek etc.) para an√°lise batch.")
upload_file = st.sidebar.file_uploader("üìÇ Upload CSV", type="csv")

###########################################################################
# Auto‚Äërefresh every 2‚ÄØs                                                  #
###########################################################################
st_autorefresh(interval=2_000, key="live_refresh")

###########################################################################
# Main layout                                                             #
###########################################################################

left, right = st.columns((2, 1))

# -------- Live traffic ---------
with left:
    st.subheader("üì° Live traffic")

    # Diagnostics
    diag_thread = st.session_state.get("capture_thread")
    st.caption(
        f"Thread alive: {diag_thread.is_alive() if diag_thread else False} | "
        f"Queue: {PACKET_QUEUE.qsize()} | "
        f"Erro: {st.session_state.get('capture_error', '')}"
    )

    rows: list[dict] = []
    while not PACKET_QUEUE.empty():
        rows.append(PACKET_QUEUE.get())
    df_live = pd.DataFrame(rows)

    if not df_live.empty:
        anomalies_live = detectar_anomalias(df_live, ["porta_origem", "porta_destino", "tamanho_pacote"])
        st.dataframe(df_live.tail(100), use_container_width=True)
        if not anomalies_live.empty:
            st.error(f"üö® {len(anomalies_live)} amea√ßas detectadas em tempo real!")
            st.dataframe(anomalies_live, use_container_width=True)
    else:
        st.info("Nenhum pacote capturado ainda‚Ä¶")

# -------- Batch CSV analysis ---------
with right:
    st.subheader("üìë Batch analysis")
    if upload_file is not None:
        try:
            df_csv = pd.read_csv(upload_file)
        except Exception as e:
            st.error(f"Falha ao ler CSV: {e}")
            df_csv = pd.DataFrame()

        if not df_csv.empty:
            missing = {"porta_origem", "porta_destino", "tamanho_pacote"} - set(df_csv.columns)
            if missing:
                st.warning(f"CSV faltando colunas {missing}. Tente mapear ou renomear antes de enviar.")
            else:
                anomalies_csv = detectar_anomalias(df_csv, ["porta_origem", "porta_destino", "tamanho_pacote"])
                st.write("Amostra dos dados:")
                st.dataframe(df_csv[df_csv.columns.difference(["score", "anomalia", "risco", "acao_recomendada"])].head(20), use_container_width=True)

                if not anomalies_csv.empty:
                    st.error(f"üö® {len(anomalies_csv)} amea√ßas encontradas no arquivo!")
                    st.dataframe(anomalies_csv, use_container_width=True)

                    relatorio = gerar_relatorio_texto(anomalies_csv)
                    st.download_button(
                        label="‚¨áÔ∏è Baixar Relat√≥rio de Amea√ßas",
                        data=relatorio,
                        file_name="relatorio_cybersentinel.txt",
                        mime="text/plain"
                    )
                else:
                    st.success("Nenhuma anomalia encontrada no arquivo enviado.")
    else:
        st.info("Carregue um CSV para an√°lise off‚Äëline.")
